# AZSII-2
# Рублёв Владимир Сергеевич
# ББМО-02-23

# Практика 2: Исследование атак на модели ИИ. Fast Gradient Sign Method (FGSM)

# Цель задания:
Изучить одну из наиболее распространённых атак на системы машинного обучения — метод Fast Gradient Sign Method (FGSM). Цель — научиться применять FGSM для генерации противоречивых примеров, способных дезориентировать обученную модель.

# Сылка на выполненую работу в среде google colab: https://colab.research.google.com/drive/17x_kbwXWYnBuGsN0VVmb0DbcT_vK5jmy#scrollTo=FRsUs3YyMMnZ

Берем сохраненную модель из прошлой практики, загружаем её вместе с тестовыми данными для последующего анализа.
![image](https://github.com/vladimirrublev/AZSII-2/blob/main/screenshot/1.png)

Теперь приступим к реализации метода FGSM для создания противоречивых примеров.

![image](https://github.com/vladimirrublev/AZSII-2/blob/main/screenshot/2.png)
![image](https://github.com/vladimirrublev/AZSII-2/blob/main/screenshot/2%201.png)

Теперь мы оценим точность модели на противоречивых изображениях, созданных с помощью FGSM.

![image](https://github.com/vladimirrublev/AZSII-2/blob/main/screenshot/3.png)

Сравнение результатов Точность на противоречивых примерах

![image](https://github.com/vladimirrublev/AZSII-2/blob/main/screenshot/4.png)

# Вывод:
В ходе выполнения работы был успешно реализован процесс создания атакующих примеров с использованием метода FGSM. Код корректно сгенерировал атакующие изображения, что позволило провести оценку точности модели как на исходных, так и на изменённых данных. Результаты показали высокую точность модели на чистых изображениях (0.9763), что подтверждает её хорошую работу. Однако, при попытке оценить точность на атакующих примерах возникла небольшая проблема, связанная с отсутствием переменной для точности на противоречивых примерах, что требует доработки. В целом, код продемонстрировал успешную работу с тестовыми данными и правильно реализовал генерацию атакующих изображений.
